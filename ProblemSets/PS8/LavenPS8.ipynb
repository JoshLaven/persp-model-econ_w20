{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Set 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1 - Decision trees (5 points).\n",
    "Joe Biden was the 47th Vice President of the United States. He was the subject of many memes, attracted the attention of Leslie Knope (Parks and Recreation, TV sitcom), and experienced a brief surge in attention due to photos from his youth. The data file biden.csv contains a selection of variables from the 2008 American National Election Studies survey that allow you to test competing factors that may influence attitudes towards Joe Biden. The variables are coded as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) \n",
    "Split the data into a training set (70%) and a test set (30%) using the sklearn.model selection.train test split() function with random state=25. Setting the seed will guarantee you all get the same results. Use recursive\n",
    "binary splitting to fit a decision tree to the training data, with biden\n",
    "as the response variable and the other variables as predictors. Set the\n",
    "max depth=3 and min samples leaf=5 Plot the tree and interpret the re-\n",
    "sults. What is the test MSE?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b) \n",
    "Use sklearn.model selection.RandomizedSearchCV to optimally tune\n",
    "the hyperparameters in the decision tree from part (a). Tune the pa- rameters max depth, min samples split, and min samples leaf. Set\n",
    "n iter=100, n jobs=-1, cv=5 for k = 5 k-fold cross validation, random state=25, and scoring=’neg mean squared error’. This last option will allow you\n",
    "to compare the MSE of the optimized tree (it will output the negative MSE) to the MSE calculated in part (a). Set your parameter distributions over which to test random combinations to the following.\n",
    "Report your optimal tuning parameter values (use the .best params ob- ject of your RandomizedSearchCV().fit(X, y)) results). Report the MSE of your optimal results (use the .best score object of your RandomizedSearchCV().fit(X, y)) results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c)\n",
    "Now tune the parameters of a RandomForest regression model on these data sklearn.ensemble.RandomForestRegressor(). Use\n",
    "sklearn.model selection.RandomizedSearchCV to optimally tune the hyperparameters in the random forest regression model. Tune the param- eters n estimators, max depth, min samples split, min samples leaf, and max features. Set n iter=100, n jobs=-1, cv=5 for k = 5 k-fold cross validation, random state=25, and scoring=’neg mean squared error’. Set your Random Forest parameter distributions over which to test random combinations to the following.\n",
    "Report your optimal tuning parameter values (use the .best params ob- ject of your RandomizedSearchCV().fit(X, y)) results). Report the MSE of your optimal results (use the .best score object of your RandomizedSearchCV().fit(X, y)) results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
